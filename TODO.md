- [ ] Support conversational context using memory store

- [ ] Persist conversation history to database

- [ ] Cache LLM responses to improve performance

- [ ] Extract LLM initialization into separate function

- [ ] Support multiple LLM backends like GPT-3

- [x] Implement Chat-style interface via TUI (Bubbletea)

- [ ] Add unit tests for core logic and error cases

- [ ] Support loading configuration from file or env

- [ ] Add logger with structured logging

- [x] Gracefully handle Ctrl+C signal to exit

- [ ] Rate limit requests to avoid hitting LLM quotas

- [ ] Add monitoring/observability (metrics, tracing)

- [ ] Create CLI with flags for region, model etc

The core functionality works well already but these could help take it to the next level in terms of quality, maintainability and production-readiness. Let me know if any particular items stand out as priorities!


